{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "333f2b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
    "try:\n",
    "    from going_modular import data_setup, engine\n",
    "except ImportError:\n",
    "    # Get the going_modular scripts\n",
    "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
    "    import os\n",
    "    os.system('git clone https://github.com/pratik-kadlak/Pytorch')\n",
    "    os.system('mv \"Pytorch/6. Going Modular\" \"going_modular\"')\n",
    "    os.system('rm -rf Pytorch')\n",
    "    from going_modular import data_setup, engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c4e88745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up device agnostic code\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# function for setting seeds\n",
    "def set_seeds(seed: int=42):\n",
    "    \"\"\"\n",
    "    Sets random sets for torch operations.\n",
    "    Args:\n",
    "        seed (int, operational): Random seed to set. Default to 42.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.mps.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e72e75f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('going_modular/data/pizza_steak_sushi/train'),\n",
       " PosixPath('going_modular/data/pizza_steak_sushi/test'))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data path\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"going_modular/data/\")\n",
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "\n",
    "# setting directory path\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6ddda03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x305f59c90>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x306804b50>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating manual transfrom and dataloaders\n",
    "\n",
    "# make transform\n",
    "from torchvision import transforms\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "manual_transform  = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "# creating dataloaders\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                               test_dir=test_dir,\n",
    "                                                                               transform=manual_transform,\n",
    "                                                                               batch_size=32)\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a272d67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x305e42c50>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x305e40c90>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating auto tranforms and dataloaders\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "\n",
    "auto_transform = weights.transforms()\n",
    "\n",
    "# creating dataloaders\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                               test_dir=test_dir,\n",
    "                                                                               transform=auto_transform,\n",
    "                                                                               batch_size=32)\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b64288b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape               Output Shape              Param #                   Trainable\n",
       "================================================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]         [32, 3]                   --                        Partial\n",
       "├─Sequential (features)                                      [32, 3, 224, 224]         [32, 1280, 7, 7]          --                        False\n",
       "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]         [32, 32, 112, 112]        --                        False\n",
       "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]         [32, 32, 112, 112]        (864)                     False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]        [32, 32, 112, 112]        (64)                      False\n",
       "│    │    └─SiLU (2)                                         [32, 32, 112, 112]        [32, 32, 112, 112]        --                        --\n",
       "│    └─Sequential (1)                                        [32, 32, 112, 112]        [32, 16, 112, 112]        --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 32, 112, 112]        [32, 16, 112, 112]        (1,448)                   False\n",
       "│    └─Sequential (2)                                        [32, 16, 112, 112]        [32, 24, 56, 56]          --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 16, 112, 112]        [32, 24, 56, 56]          (6,004)                   False\n",
       "│    │    └─MBConv (1)                                       [32, 24, 56, 56]          [32, 24, 56, 56]          (10,710)                  False\n",
       "│    └─Sequential (3)                                        [32, 24, 56, 56]          [32, 40, 28, 28]          --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 24, 56, 56]          [32, 40, 28, 28]          (15,350)                  False\n",
       "│    │    └─MBConv (1)                                       [32, 40, 28, 28]          [32, 40, 28, 28]          (31,290)                  False\n",
       "│    └─Sequential (4)                                        [32, 40, 28, 28]          [32, 80, 14, 14]          --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 40, 28, 28]          [32, 80, 14, 14]          (37,130)                  False\n",
       "│    │    └─MBConv (1)                                       [32, 80, 14, 14]          [32, 80, 14, 14]          (102,900)                 False\n",
       "│    │    └─MBConv (2)                                       [32, 80, 14, 14]          [32, 80, 14, 14]          (102,900)                 False\n",
       "│    └─Sequential (5)                                        [32, 80, 14, 14]          [32, 112, 14, 14]         --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 80, 14, 14]          [32, 112, 14, 14]         (126,004)                 False\n",
       "│    │    └─MBConv (1)                                       [32, 112, 14, 14]         [32, 112, 14, 14]         (208,572)                 False\n",
       "│    │    └─MBConv (2)                                       [32, 112, 14, 14]         [32, 112, 14, 14]         (208,572)                 False\n",
       "│    └─Sequential (6)                                        [32, 112, 14, 14]         [32, 192, 7, 7]           --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 112, 14, 14]         [32, 192, 7, 7]           (262,492)                 False\n",
       "│    │    └─MBConv (1)                                       [32, 192, 7, 7]           [32, 192, 7, 7]           (587,952)                 False\n",
       "│    │    └─MBConv (2)                                       [32, 192, 7, 7]           [32, 192, 7, 7]           (587,952)                 False\n",
       "│    │    └─MBConv (3)                                       [32, 192, 7, 7]           [32, 192, 7, 7]           (587,952)                 False\n",
       "│    └─Sequential (7)                                        [32, 192, 7, 7]           [32, 320, 7, 7]           --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 192, 7, 7]           [32, 320, 7, 7]           (717,232)                 False\n",
       "│    └─Conv2dNormActivation (8)                              [32, 320, 7, 7]           [32, 1280, 7, 7]          --                        False\n",
       "│    │    └─Conv2d (0)                                       [32, 320, 7, 7]           [32, 1280, 7, 7]          (409,600)                 False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 7]          [32, 1280, 7, 7]          (2,560)                   False\n",
       "│    │    └─SiLU (2)                                         [32, 1280, 7, 7]          [32, 1280, 7, 7]          --                        --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]          [32, 1280, 1, 1]          --                        --\n",
       "├─Sequential (classifier)                                    [32, 1280]                [32, 3]                   --                        True\n",
       "│    └─Dropout (0)                                           [32, 1280]                [32, 1280]                --                        --\n",
       "│    └─Linear (1)                                            [32, 1280]                [32, 3]                   3,843                     True\n",
       "================================================================================================================================================================\n",
       "Total params: 4,011,391\n",
       "Trainable params: 3,843\n",
       "Non-trainable params: 4,007,548\n",
       "Total mult-adds (Units.GIGABYTES): 12.31\n",
       "================================================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3452.09\n",
       "Params size (MB): 16.05\n",
       "Estimated Total Size (MB): 3487.41\n",
       "================================================================================================================================================================"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the pretrained model\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "effnetb0 = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
    "\n",
    "# froze all the layers\n",
    "for param in effnetb0.features.parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "# we have to update the last layer\n",
    "from torch import nn\n",
    "\n",
    "set_seeds(42)\n",
    "\n",
    "effnetb0.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280,\n",
    "              out_features=len(class_names))\n",
    ").to(device)\n",
    "\n",
    "# print the summary of the model\n",
    "from torchinfo import summary\n",
    "summary(effnetb0,\n",
    "        input_size=(32, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f06a367",
   "metadata": {},
   "source": [
    "## Training and Tracking Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01db8a21",
   "metadata": {},
   "source": [
    "For tracking results we use __tensorboard -> https://pytorch.org/docs/stable/tensorboard.html__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c1b5fa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up loss_fn \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# setting up optimizer\n",
    "optimizer = torch.optim.Adam(params=effnetb0.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bcb64ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.tensorboard.writer.SummaryWriter at 0x305e435d0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up a summary_writer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "250ea70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tracking the result we have to write the results to the file so we have to change the train() func\n",
    "\n",
    "# there is no change in train_step and test_step so we can use them directly\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          loss_fn: nn.Module,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          device,\n",
    "          epochs: int = 5):\n",
    "    \"\"\"\n",
    "    Trains the given model using the specified data loaders, loss function, optimizer, and device.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train.\n",
    "        train_dataloader (torch.utils.data.DataLoader): The DataLoader providing the training data.\n",
    "        test_dataloader (torch.utils.data.DataLoader): The DataLoader providing the test data.\n",
    "        loss_fn (nn.Module): The loss function to use.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer to use for updating model parameters.\n",
    "        device (torch.device): The device to run the training on (e.g., 'cpu' or 'cuda').\n",
    "        epochs (int, optional): The number of epochs to train the model (default is 5).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the training and testing results including losses and accuracies.\n",
    "    \"\"\"\n",
    "    \n",
    "    # send the model to target device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # creating a empty result dictionary\n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = engine.train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
    "        test_loss, test_acc = engine.test_step(model, test_dataloader, loss_fn, device)\n",
    "        print(f\"Epoch:{epoch}\\tTrain_Loss:{train_loss:.4f}\\tTrain_Acc:{train_acc:.4f}\\tTest_Loss:{test_loss:.4f}\\tTest_Acc:{test_acc:.4f}\")\n",
    "\n",
    "        # update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "        \n",
    "        # experiment tracking\n",
    "        writer.add_scalars(main_tag=\"Loss\",\n",
    "                           tag_scalar_dict={\"train_loss\": train_loss,\n",
    "                                            \"test_loss\": test_loss},\n",
    "                           global_step=epoch)\n",
    "        \n",
    "        writer.add_scalars(main_tag=\"Accuracy\",\n",
    "                           tag_scalar_dict={\"train_acc\": train_acc,\n",
    "                                            \"test_acc\": test_acc},\n",
    "                           global_step=epoch)\n",
    "        \n",
    "        writer.add_graph(model=effnet_model,\n",
    "                         input_to_model=torch.randn(32, 3, 224, 224).to(device))\n",
    "        \n",
    "        writer.close()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1c7c35ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\tTrain_Loss:1.0823\tTrain_Acc:40.6250\tTest_Loss:0.8991\tTest_Acc:59.0909\n",
      "Epoch:1\tTrain_Loss:0.9203\tTrain_Acc:66.7969\tTest_Loss:0.7960\tTest_Acc:84.5644\n",
      "Epoch:2\tTrain_Loss:0.7825\tTrain_Acc:75.7812\tTest_Loss:0.6825\tTest_Acc:88.6364\n",
      "Epoch:3\tTrain_Loss:0.6894\tTrain_Acc:84.7656\tTest_Loss:0.6784\tTest_Acc:84.5644\n",
      "Epoch:4\tTrain_Loss:0.7096\tTrain_Acc:71.4844\tTest_Loss:0.6878\tTest_Acc:71.3068\n"
     ]
    }
   ],
   "source": [
    "# training model\n",
    "set_seeds()\n",
    "results = train(model=effnetb0,\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                optimizer=optimizer,\n",
    "                loss_fn=loss_fn,\n",
    "                epochs=5,\n",
    "                device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3399cb",
   "metadata": {},
   "source": [
    "## View Log Files with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b28c31e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1651), started 10:08:57 ago. (Use '!kill 1651' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ce436d9d3cc2f6de\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ce436d9d3cc2f6de\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's view our experiments from within the notebook\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e92ea6",
   "metadata": {},
   "source": [
    "## Function for tracking results\n",
    "By default out `SummaryWriter()` class saves to `log_dir`. <br>\n",
    "How about if we wanted to save different experiments to different folders?  <br>\n",
    "In essence, one experiment = one folder  <br>\n",
    "<br>\n",
    "For example, we'd like to track:\n",
    "* Experiment data/timestamp\n",
    "* Experiment name\n",
    "* Model Name\n",
    "* Extra - is there anything else that should be tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ed98bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_writer(experiment_name: str,\n",
    "                  model_name: str, \n",
    "                  extra: str = None):\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # get current timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # setting the log folder name \n",
    "    if extra: \n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n",
    "    else:\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
    "        \n",
    "    print(f\"[INFO] Created SummaryWriter savign to {log_dir}\")\n",
    "    \n",
    "    return SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c387cbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created SummaryWriter savign to runs/2024-06-25/data_10_percent/effnetb0/epochs_5_optim_Adam\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.tensorboard.writer.SummaryWriter at 0x3dfb28090>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_writer = create_writer(experiment_name=\"data_10_percent\",\n",
    "                            model_name=\"effnetb0\",\n",
    "                            extra=\"epochs_5_optim_Adam\")\n",
    "\n",
    "demo_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c9e884e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating train_func() more so that it can use create_writer() func and save the results in the folder\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          loss_fn: nn.Module,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          device,\n",
    "          writer: torch.utils.tensorboard.writer.SummaryWriter,\n",
    "          epochs: int = 5):\n",
    "    \"\"\"\n",
    "    Trains the given model using the specified data loaders, loss function, optimizer, and device.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train.\n",
    "        train_dataloader (torch.utils.data.DataLoader): The DataLoader providing the training data.\n",
    "        test_dataloader (torch.utils.data.DataLoader): The DataLoader providing the test data.\n",
    "        loss_fn (nn.Module): The loss function to use.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer to use for updating model parameters.\n",
    "        device (torch.device): The device to run the training on (e.g., 'cpu' or 'cuda').\n",
    "        log_dir (str): Directory to save the results of the experiment\n",
    "        epochs (int, optional): The number of epochs to train the model (default is 5).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the training and testing results including losses and accuracies.\n",
    "    \"\"\"\n",
    "    \n",
    "    # send the model to target device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # creating a empty result dictionary\n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = engine.train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
    "        test_loss, test_acc = engine.test_step(model, test_dataloader, loss_fn, device)\n",
    "        print(f\"Epoch:{epoch}\\tTrain_Loss:{train_loss:.4f}\\tTrain_Acc:{train_acc:.4f}\\tTest_Loss:{test_loss:.4f}\\tTest_Acc:{test_acc:.4f}\")\n",
    "\n",
    "        # update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "        \n",
    "        # experiment tracking\n",
    "        if writer:\n",
    "            writer.add_scalars(main_tag=\"Loss\",\n",
    "                               tag_scalar_dict={\"train_loss\": train_loss,\n",
    "                                                \"test_loss\": test_loss},\n",
    "                               global_step=epoch)\n",
    "\n",
    "            writer.add_scalars(main_tag=\"Accuracy\",\n",
    "                               tag_scalar_dict={\"train_acc\": train_acc,\n",
    "                                                \"test_acc\": test_acc},\n",
    "                               global_step=epoch)\n",
    "\n",
    "            writer.add_graph(model=effnet_model,\n",
    "                             input_to_model=torch.randn(32, 3, 224, 224).to(device))\n",
    "\n",
    "            writer.close()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e30f91",
   "metadata": {},
   "source": [
    "## Running Various Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b63e65",
   "metadata": {},
   "source": [
    "For Running the experiments we will need 2 datasets\n",
    "1. pizza, steak, sushi - 10% -> https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\n",
    "2. pizza, steak, sushi - 20% -> https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "064053df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Downloading data from https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip...\n",
      "[INFO] Extracting data to pizza_steak_sushi...\n",
      "[INFO] Data downloaded and extracted to pizza_steak_sushi.\n",
      "[INFO] Downloading data from https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip...\n",
      "[INFO] Extracting data to pizza_steak_sushi_20_percent...\n",
      "[INFO] Data downloaded and extracted to pizza_steak_sushi_20_percent.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "def download_data(source: str, destination: str):\n",
    "    \"\"\"\n",
    "    Downloads a zip file from the specified source URL and extracts it to the specified destination directory.\n",
    "\n",
    "    Parameters:\n",
    "    source (str): URL of the zip file to be downloaded.\n",
    "    destination (str): Directory where the zip file should be extracted.\n",
    "    \"\"\"\n",
    "    # Check if the destination directory already exists\n",
    "    if not os.path.exists(destination):\n",
    "        os.makedirs(destination)\n",
    "\n",
    "    # Download the file\n",
    "    print(f\"[INFO] Downloading data from {source}...\")\n",
    "    response = requests.get(source)\n",
    "    zip_path = os.path.join(destination, \"data.zip\")\n",
    "\n",
    "    # Save the zip file\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    # Extract the zip file\n",
    "    print(f\"[INFO] Extracting data to {destination}...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(destination)\n",
    "    \n",
    "    # Remove the zip file after extraction\n",
    "    os.remove(zip_path)\n",
    "    print(f\"[INFO] Data downloaded and extracted to {destination}.\")\n",
    "    \n",
    "\n",
    "\n",
    "# downloading both datasets\n",
    "download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "              destination=\"pizza_steak_sushi\")\n",
    "\n",
    "download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
    "              destination=\"pizza_steak_sushi_20_percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7b57d",
   "metadata": {},
   "source": [
    "__Transorming Datasets and creating DataLoaders__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e18a842f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('pizza_steak_sushi/train'),\n",
       " PosixPath('pizza_steak_sushi_20_percent/train'),\n",
       " PosixPath('pizza_steak_sushi/test'))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data path\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "data_10_percent_path = Path(\"pizza_steak_sushi/\")\n",
    "data_20_percent_path = Path(\"pizza_steak_sushi_20_percent/\")\n",
    "\n",
    "# data_path = Path(\"going_modular/data/\")\n",
    "\n",
    "# training dataset paths\n",
    "train_dir_10_percent = data_10_percent_path / \"train\"\n",
    "train_dir_20_percent = data_20_percent_path / \"train\"\n",
    "\n",
    "# we will have only one test dataset so that the comparison can be fair\n",
    "test_dir_10_percent = data_10_percent_path / \"test\"\n",
    "\n",
    "train_dir_10_percent, train_dir_20_percent, test_dir_10_percent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "01838f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataloader_10_percent num_batches: 8\n",
      "train_dataloader_20_percent num_batches: 15\n",
      "test_dataloader num_batches: 3\n"
     ]
    }
   ],
   "source": [
    "# create transform\n",
    "from torchvision import transforms\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "manual_transform  = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "# creating dataloaders\n",
    "train_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,\n",
    "                                                                                          test_dir=test_dir_10_percent,\n",
    "                                                                                          transform=manual_transform,\n",
    "                                                                                          batch_size=32)\n",
    "\n",
    "train_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,\n",
    "                                                                                          test_dir=test_dir_10_percent,\n",
    "                                                                                          transform=manual_transform,\n",
    "                                                                                          batch_size=32)\n",
    "\n",
    "print(f\"train_dataloader_10_percent num_batches: {len(train_dataloader_10_percent)}\")\n",
    "print(f\"train_dataloader_20_percent num_batches: {len(train_dataloader_20_percent)}\")\n",
    "print(f\"test_dataloader num_batches: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ea32fca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an effnetb0 feature extractor\n",
    "def create_effnetb0():\n",
    "    # loading the model\n",
    "    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
    "    \n",
    "    # freeze all the layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad=False\n",
    "    \n",
    "    set_seeds()\n",
    "    \n",
    "    # creating last layer compatible\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=True),\n",
    "        nn.Linear(in_features=1280,\n",
    "                  out_features=len(class_names))\n",
    "    ).to(device)\n",
    "    \n",
    "    # giving model a name\n",
    "    model.name = \"effnetb0\"\n",
    "    print(f\"[INFO] Created new {model.name} model...\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "937cac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_effnetb2():\n",
    "    # loading the model\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n",
    "    \n",
    "    # freeze all the layers\n",
    "    for params in model.features.parameters():\n",
    "        params.requires_grad=False\n",
    "        \n",
    "    # set random seeds\n",
    "    set_seeds()\n",
    "    \n",
    "    # creating last layers compatible\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408,\n",
    "                  out_features=len(class_names))\n",
    "    ).to(device)\n",
    "    \n",
    "    # giving model a name\n",
    "    model.name = \"effnetb2\"\n",
    "    print(f\"[INFO] Created new {model.name} model...\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7702c8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created new effnetb0 model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "================================================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape               Output Shape              Param #                   Trainable\n",
       "================================================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]         [32, 3]                   --                        Partial\n",
       "├─Sequential (features)                                      [32, 3, 224, 224]         [32, 1280, 7, 7]          --                        False\n",
       "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]         [32, 32, 112, 112]        --                        False\n",
       "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]         [32, 32, 112, 112]        (864)                     False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]        [32, 32, 112, 112]        (64)                      False\n",
       "│    │    └─SiLU (2)                                         [32, 32, 112, 112]        [32, 32, 112, 112]        --                        --\n",
       "│    └─Sequential (1)                                        [32, 32, 112, 112]        [32, 16, 112, 112]        --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 32, 112, 112]        [32, 16, 112, 112]        (1,448)                   False\n",
       "│    └─Sequential (2)                                        [32, 16, 112, 112]        [32, 24, 56, 56]          --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 16, 112, 112]        [32, 24, 56, 56]          (6,004)                   False\n",
       "│    │    └─MBConv (1)                                       [32, 24, 56, 56]          [32, 24, 56, 56]          (10,710)                  False\n",
       "│    └─Sequential (3)                                        [32, 24, 56, 56]          [32, 40, 28, 28]          --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 24, 56, 56]          [32, 40, 28, 28]          (15,350)                  False\n",
       "│    │    └─MBConv (1)                                       [32, 40, 28, 28]          [32, 40, 28, 28]          (31,290)                  False\n",
       "│    └─Sequential (4)                                        [32, 40, 28, 28]          [32, 80, 14, 14]          --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 40, 28, 28]          [32, 80, 14, 14]          (37,130)                  False\n",
       "│    │    └─MBConv (1)                                       [32, 80, 14, 14]          [32, 80, 14, 14]          (102,900)                 False\n",
       "│    │    └─MBConv (2)                                       [32, 80, 14, 14]          [32, 80, 14, 14]          (102,900)                 False\n",
       "│    └─Sequential (5)                                        [32, 80, 14, 14]          [32, 112, 14, 14]         --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 80, 14, 14]          [32, 112, 14, 14]         (126,004)                 False\n",
       "│    │    └─MBConv (1)                                       [32, 112, 14, 14]         [32, 112, 14, 14]         (208,572)                 False\n",
       "│    │    └─MBConv (2)                                       [32, 112, 14, 14]         [32, 112, 14, 14]         (208,572)                 False\n",
       "│    └─Sequential (6)                                        [32, 112, 14, 14]         [32, 192, 7, 7]           --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 112, 14, 14]         [32, 192, 7, 7]           (262,492)                 False\n",
       "│    │    └─MBConv (1)                                       [32, 192, 7, 7]           [32, 192, 7, 7]           (587,952)                 False\n",
       "│    │    └─MBConv (2)                                       [32, 192, 7, 7]           [32, 192, 7, 7]           (587,952)                 False\n",
       "│    │    └─MBConv (3)                                       [32, 192, 7, 7]           [32, 192, 7, 7]           (587,952)                 False\n",
       "│    └─Sequential (7)                                        [32, 192, 7, 7]           [32, 320, 7, 7]           --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 192, 7, 7]           [32, 320, 7, 7]           (717,232)                 False\n",
       "│    └─Conv2dNormActivation (8)                              [32, 320, 7, 7]           [32, 1280, 7, 7]          --                        False\n",
       "│    │    └─Conv2d (0)                                       [32, 320, 7, 7]           [32, 1280, 7, 7]          (409,600)                 False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 7]          [32, 1280, 7, 7]          (2,560)                   False\n",
       "│    │    └─SiLU (2)                                         [32, 1280, 7, 7]          [32, 1280, 7, 7]          --                        --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]          [32, 1280, 1, 1]          --                        --\n",
       "├─Sequential (classifier)                                    [32, 1280]                [32, 3]                   --                        True\n",
       "│    └─Dropout (0)                                           [32, 1280]                [32, 1280]                --                        --\n",
       "│    └─Linear (1)                                            [32, 1280]                [32, 3]                   3,843                     True\n",
       "================================================================================================================================================================\n",
       "Total params: 4,011,391\n",
       "Trainable params: 3,843\n",
       "Non-trainable params: 4,007,548\n",
       "Total mult-adds (Units.GIGABYTES): 12.31\n",
       "================================================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3452.09\n",
       "Params size (MB): 16.05\n",
       "Estimated Total Size (MB): 3487.41\n",
       "================================================================================================================================================================"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effnetb0 = create_effnetb0()\n",
    "summary(effnetb0,\n",
    "        input_size=(32, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a9722d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created new effnetb2 model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "================================================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape               Output Shape              Param #                   Trainable\n",
       "================================================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]         [32, 3]                   --                        Partial\n",
       "├─Sequential (features)                                      [32, 3, 224, 224]         [32, 1408, 7, 7]          --                        False\n",
       "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]         [32, 32, 112, 112]        --                        False\n",
       "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]         [32, 32, 112, 112]        (864)                     False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]        [32, 32, 112, 112]        (64)                      False\n",
       "│    │    └─SiLU (2)                                         [32, 32, 112, 112]        [32, 32, 112, 112]        --                        --\n",
       "│    └─Sequential (1)                                        [32, 32, 112, 112]        [32, 16, 112, 112]        --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 32, 112, 112]        [32, 16, 112, 112]        (1,448)                   False\n",
       "│    │    └─MBConv (1)                                       [32, 16, 112, 112]        [32, 16, 112, 112]        (612)                     False\n",
       "│    └─Sequential (2)                                        [32, 16, 112, 112]        [32, 24, 56, 56]          --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 16, 112, 112]        [32, 24, 56, 56]          (6,004)                   False\n",
       "│    │    └─MBConv (1)                                       [32, 24, 56, 56]          [32, 24, 56, 56]          (10,710)                  False\n",
       "│    │    └─MBConv (2)                                       [32, 24, 56, 56]          [32, 24, 56, 56]          (10,710)                  False\n",
       "│    └─Sequential (3)                                        [32, 24, 56, 56]          [32, 48, 28, 28]          --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 24, 56, 56]          [32, 48, 28, 28]          (16,518)                  False\n",
       "│    │    └─MBConv (1)                                       [32, 48, 28, 28]          [32, 48, 28, 28]          (43,308)                  False\n",
       "│    │    └─MBConv (2)                                       [32, 48, 28, 28]          [32, 48, 28, 28]          (43,308)                  False\n",
       "│    └─Sequential (4)                                        [32, 48, 28, 28]          [32, 88, 14, 14]          --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 48, 28, 28]          [32, 88, 14, 14]          (50,300)                  False\n",
       "│    │    └─MBConv (1)                                       [32, 88, 14, 14]          [32, 88, 14, 14]          (123,750)                 False\n",
       "│    │    └─MBConv (2)                                       [32, 88, 14, 14]          [32, 88, 14, 14]          (123,750)                 False\n",
       "│    │    └─MBConv (3)                                       [32, 88, 14, 14]          [32, 88, 14, 14]          (123,750)                 False\n",
       "│    └─Sequential (5)                                        [32, 88, 14, 14]          [32, 120, 14, 14]         --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 88, 14, 14]          [32, 120, 14, 14]         (149,158)                 False\n",
       "│    │    └─MBConv (1)                                       [32, 120, 14, 14]         [32, 120, 14, 14]         (237,870)                 False\n",
       "│    │    └─MBConv (2)                                       [32, 120, 14, 14]         [32, 120, 14, 14]         (237,870)                 False\n",
       "│    │    └─MBConv (3)                                       [32, 120, 14, 14]         [32, 120, 14, 14]         (237,870)                 False\n",
       "│    └─Sequential (6)                                        [32, 120, 14, 14]         [32, 208, 7, 7]           --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 120, 14, 14]         [32, 208, 7, 7]           (301,406)                 False\n",
       "│    │    └─MBConv (1)                                       [32, 208, 7, 7]           [32, 208, 7, 7]           (686,868)                 False\n",
       "│    │    └─MBConv (2)                                       [32, 208, 7, 7]           [32, 208, 7, 7]           (686,868)                 False\n",
       "│    │    └─MBConv (3)                                       [32, 208, 7, 7]           [32, 208, 7, 7]           (686,868)                 False\n",
       "│    │    └─MBConv (4)                                       [32, 208, 7, 7]           [32, 208, 7, 7]           (686,868)                 False\n",
       "│    └─Sequential (7)                                        [32, 208, 7, 7]           [32, 352, 7, 7]           --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 208, 7, 7]           [32, 352, 7, 7]           (846,900)                 False\n",
       "│    │    └─MBConv (1)                                       [32, 352, 7, 7]           [32, 352, 7, 7]           (1,888,920)               False\n",
       "│    └─Conv2dNormActivation (8)                              [32, 352, 7, 7]           [32, 1408, 7, 7]          --                        False\n",
       "│    │    └─Conv2d (0)                                       [32, 352, 7, 7]           [32, 1408, 7, 7]          (495,616)                 False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 1408, 7, 7]          [32, 1408, 7, 7]          (2,816)                   False\n",
       "│    │    └─SiLU (2)                                         [32, 1408, 7, 7]          [32, 1408, 7, 7]          --                        --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [32, 1408, 7, 7]          [32, 1408, 1, 1]          --                        --\n",
       "├─Sequential (classifier)                                    [32, 1408]                [32, 3]                   --                        True\n",
       "│    └─Dropout (0)                                           [32, 1408]                [32, 1408]                --                        --\n",
       "│    └─Linear (1)                                            [32, 1408]                [32, 3]                   4,227                     True\n",
       "================================================================================================================================================================\n",
       "Total params: 7,705,221\n",
       "Trainable params: 4,227\n",
       "Non-trainable params: 7,700,994\n",
       "Total mult-adds (Units.GIGABYTES): 21.04\n",
       "================================================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 5017.53\n",
       "Params size (MB): 30.82\n",
       "Estimated Total Size (MB): 5067.62\n",
       "================================================================================================================================================================"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effnetb2 = create_effnetb2()\n",
    "summary(effnetb2,\n",
    "        input_size=(32, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8887d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
